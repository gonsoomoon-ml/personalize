{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 5.1] getRec() 통한 추천 리스트 얻기\n",
    "\n",
    "이 노트북은 Module2에서 생성한 솔류션을 바탕으로 아래와 같은 작업을 합니다.\n",
    "* 캠페인 생성\n",
    "* 캠페인을 통해 특정 유저에 대한 추천 영화 리스트 얻기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 여러분의 환경이 Amazon Personalize와 성공적으로 통신할 수 있는지 확인해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드 셀은 이전 notebook에서 저장했던 공유 변수들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성할 오브젝트의 끝에 임의의 숫자를 부여하기 위해 suffix 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 샘플 추천 결과 얻기\n",
    "\n",
    "캠페인이 활성화되면 추천 결과를 받을 수 있습니다. 먼저 컬렉션에서 임의의 사용자를 선택해야 합니다. 그런 다음, ID 대신 추천을 위해 영화 정보를 표시하는 몇 가지 헬퍼 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>GENRE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>3948</td>\n",
       "      <td>Meet the Parents (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>3949</td>\n",
       "      <td>Requiem for a Dream (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>3950</td>\n",
       "      <td>Tigerland (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>3951</td>\n",
       "      <td>Two Family House (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>3952</td>\n",
       "      <td>Contender, The (2000)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ITEM_ID                       TITLE           GENRE\n",
       "3878     3948     Meet the Parents (2000)          Comedy\n",
       "3879     3949  Requiem for a Dream (2000)           Drama\n",
       "3880     3950            Tigerland (2000)           Drama\n",
       "3881     3951     Two Family House (2000)           Drama\n",
       "3882     3952       Contender, The (2000)  Drama|Thriller"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_all = pd.read_csv('./ml-1m/movies.dat',sep='::', encoding='latin1',names=['ITEM_ID', 'TITLE', 'GENRE'],)\n",
    "items=items_all.copy()\n",
    "\n",
    "items['to_keep'] = items['ITEM_ID'].apply(lambda x:x in unique_items)\n",
    "items=items[items['to_keep']]\n",
    "del items['to_keep']\n",
    "items.tail()\n",
    "\n",
    "#len(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_title(movie_id):\n",
    "    \"\"\"\n",
    "    Takes in an ID, returns a title\n",
    "    \"\"\"\n",
    "    movie_id = int(movie_id)\n",
    "    movie_title=items[items['ITEM_ID']==movie_id]['TITLE']\n",
    "    return (movie_title.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HRNN GetRecommendations 호출\n",
    "\n",
    "아래 코드 셀을 실행하면 특정 사용자에 대한 추천 사항이 표시되고 추천 영화 목록이 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(interaction_filename)\n",
    "\n",
    "# # Getting a random user:\n",
    "# user_id, item_id, _,_,_ = df.sample().values[0]\n",
    "# print(\"USER: {}\".format(user_id))\n",
    "\n",
    "# get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "#     campaignArn = hrnn_campaign_arn,\n",
    "#     userId = str(user_id),\n",
    "# )\n",
    "# # Update DF rendering\n",
    "# pd.set_option('display.max_rows', 30)\n",
    "\n",
    "# print(\"Recommendations for user: \", user_id)\n",
    "\n",
    "# item_list = get_recommendations_response['itemList']\n",
    "# recommendation_title_list = []\n",
    "# recommendation_id_list=[]\n",
    "# for item in item_list:\n",
    "#     title = get_movie_title(item['itemId'])\n",
    "#     recommendation_title_list.append(title)\n",
    "#     recommendation_id_list.append(item['itemId'])\n",
    "# recommendations_df = pd.DataFrame(recommendation_title_list, columns = ['OriginalRecs'])\n",
    "# recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sims GetRecommendations 호출\n",
    "아래 코드 셀을 실행하면 특정 아이템과 유사한 추천 영화 목록이 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting a random user:\n",
    "# user_id, item_id, _,_,_ = df.sample().values[0]\n",
    "# print(\"ITEM ID: {}\".format(item_id))\n",
    "\n",
    "\n",
    "# get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "#     campaignArn = sims_campaign_arn,\n",
    "#     itemId = str(item_id),\n",
    "# )\n",
    "# # Update DF rendering\n",
    "# pd.set_option('display.max_rows', 30)\n",
    "\n",
    "# print(\"Recommendations for item_id: \", item_id)\n",
    "\n",
    "# item_list = get_recommendations_response['itemList']\n",
    "# recommendation_title_list = []\n",
    "# recommendation_id_list=[]\n",
    "# for item in item_list:\n",
    "#     title = get_movie_title(item['itemId'])\n",
    "#     recommendation_title_list.append(title)\n",
    "#     recommendation_id_list.append(item['itemId'])\n",
    "# recommendations_df = pd.DataFrame(recommendation_title_list, columns = ['OriginalRecs'])\n",
    "# recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Personalize Batch Export 작업 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Amazon Personalize Batch기능을 활용하려면 json 파일 형식으로 추천 받고하 자는 사용자 또는 아이템 아이디를 json 형태의 파일로 s3에 저장하여야 합니다. Output의 형식도 json형태로 저장되며 지정한 S3 bucket 경로에 저장 되게 됩니다. \n",
    "\n",
    "HRNN 솔루션  Batch Input 예제: \n",
    "\n",
    "```JSON,\n",
    "    {\"userId\": \"4638\"},\n",
    "    {\"userId\": \"663\"},\n",
    "    {\"userId\": \"3384\"},\n",
    "```\n",
    "\n",
    "\n",
    "Batch Output 예제: \n",
    "```JSON,\n",
    "{\"input\":{\"userId\":\"4638\"}, \"output\": {\"recommendedItems\": [\"296\", \"1\", \"260\", \"318\"]}}\n",
    "{\"input\":{\"userId\":\"663\"}, \"output\": {\"recommendedItems\": [\"1393\", \"3793\", \"2701\", \"3826\"]}}\n",
    "{\"input\":{\"userId\":\"3384\"}, \"output\": {\"recommendedItems\": [\"8368\", \"5989\", \"40815\", \"48780\"]}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the user list\n",
    "df=pd.read_csv(warm_train_interaction_filename)\n",
    "df_users = df['USER_ID'].unique()\n",
    "df_users=pd.DataFrame(df_users,columns=['USER_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the user list\n",
    "#batch_users = df_users.sample(3).index.tolist()\n",
    "batch_users=df_users.index.tolist()\n",
    "data_dir=\"dataset/\"\n",
    "# Write the file to disk\n",
    "json_input_filename = \"json_input.json\"\n",
    "with open(data_dir+json_input_filename, 'w') as json_input:\n",
    "    for user_id in batch_users:\n",
    "        json_input.write('{\"userId\": \"' + str(user_id) + '\"}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"userId\": \"0\"}\n",
      "{\"userId\": \"1\"}\n",
      "{\"userId\": \"2\"}\n",
      "{\"userId\": \"3\"}\n",
      "{\"userId\": \"4\"}\n"
     ]
    }
   ],
   "source": [
    "# Showcase the input file:\n",
    "!head -n5 $data_dir$json_input_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"userId\": \"6035\"}\n",
      "{\"userId\": \"6036\"}\n",
      "{\"userId\": \"6037\"}\n",
      "{\"userId\": \"6038\"}\n",
      "{\"userId\": \"6039\"}\n"
     ]
    }
   ],
   "source": [
    "!tail -n5 $data_dir$json_input_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-057716757052/dataset/json_input.json\n"
     ]
    }
   ],
   "source": [
    "# Upload files to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(data_dir+json_input_filename).upload_file(data_dir+json_input_filename)\n",
    "s3_input_path = \"s3://\" + bucket + \"/\" + data_dir+json_input_filename\n",
    "print(s3_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-057716757052/dataset/\n"
     ]
    }
   ],
   "source": [
    "# Define the output path\n",
    "s3_output_path = \"s3://\" + bucket + \"/\"+data_dir\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::057716757052:role/PersonalizeRoleDemo91891\n"
     ]
    }
   ],
   "source": [
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchInferenceJobArn = personalize.create_batch_inference_job (\n",
    "    solutionVersionArn = hrnn_solution_version_arn,\n",
    "    jobName = \"POC-Batch-Inference-Job-HRNN-\"+suffix,\n",
    "    roleArn = role_arn,\n",
    "    jobInput = \n",
    "     {\"s3DataSource\": {\"path\": s3_input_path}},\n",
    "    jobOutput = \n",
    "     {\"s3DataDestination\":{\"path\": s3_output_path}}\n",
    ")\n",
    "batchInferenceJobArn = batchInferenceJobArn['batchInferenceJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Started on:  12:58:25 PM\n",
      "DatasetInferenceJob: CREATE PENDING\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: CREATE IN_PROGRESS\n",
      "DatasetInferenceJob: ACTIVE\n",
      "Import Completed on:  01:24:28 PM\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now()\n",
    "print(\"Import Started on: \", current_time.strftime(\"%I:%M:%S %p\"))\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_inference_job_response = personalize.describe_batch_inference_job(\n",
    "        batchInferenceJobArn = batchInferenceJobArn\n",
    "    )\n",
    "    status = describe_dataset_inference_job_response[\"batchInferenceJob\"]['status']\n",
    "    print(\"DatasetInferenceJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "current_time = datetime.now()\n",
    "print(\"Import Completed on: \", current_time.strftime(\"%I:%M:%S %p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "export_name = json_input_filename + \".out\"\n",
    "s3.download_file(bucket,data_dir+export_name,data_dir+export_name)\n",
    "\n",
    "# Update DF rendering\n",
    "pd.set_option('display.max_rows', 30)\n",
    "with open(\"dataset/\"+export_name) as json_file:\n",
    "    # Get the first line and parse it\n",
    "    line = json.loads(json_file.readline())\n",
    "    # Do the same for the other lines\n",
    "    while line:\n",
    "        # extract the user ID \n",
    "        col_header = \"User: \" + line['input']['userId']\n",
    "        # Create a list for all the artists\n",
    "        recommendation_list = []\n",
    "        # Add all the entries\n",
    "        for item in line['output']['recommendedItems']:\n",
    "            title = get_movie_title(item)\n",
    "            recommendation_list.append(title)\n",
    "        if 'bulk_recommendations_df' in locals():\n",
    "            new_rec_DF = pd.DataFrame(recommendation_list, columns = [col_header])\n",
    "            bulk_recommendations_df = bulk_recommendations_df.join(new_rec_DF)\n",
    "        else:\n",
    "            bulk_recommendations_df = pd.DataFrame(recommendation_list, columns=[col_header])\n",
    "        try:\n",
    "            line = json.loads(json_file.readline())\n",
    "        except:\n",
    "            line = None\n",
    "bulk_recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리뷰\n",
    "\n",
    "캠페인을 생성하고 실제적으로 특정 유저의 추천 영화 목록도 얻었습니다.\n",
    "이제 다음 노트북으로 넘어갈 준비가 되었습니다. (`4.View_Campaign_And_Interactions.ipynb`)\n",
    "\n",
    "\n",
    "## 다음 노트북에 대한 참고 사항\n",
    "\n",
    "다음 실습에 필요한 몇 가지 값들이 있습니다. 아래 셀을 실행하여 저장한 후, 다음 주피터 노트북에서 그대로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store recommendations_df\n",
    "%store bulk_recommendations_df\n",
    "%store user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
